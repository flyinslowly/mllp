{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The approximation problem\n",
    "\n",
    "Let $f:X\\to R$ be a function and P be an finite subset of X. Define by $f|_P$ restriction $f$ onto subset $P$.\n",
    "\n",
    "The problem is to find an smoothing operator  $S:f|_P \\mapsto \\tilde{f}$ such that $f$ and $\\tilde{f}=S(f|_P)$ are close to each other\n",
    "\n",
    "So we have the following diagram\n",
    "$$\n",
    "f\\mapsto f|_P \\mapsto \\tilde{f} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active learning problem: \n",
    "\n",
    "Find $x^*$ such that \n",
    "$$\n",
    "dist(f, S(f|_{P\\cup \\{x^*\\}})) \\mapsto min_{x^*}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian process\n",
    "If f is GP then we can define:\n",
    "$$\n",
    "S(f|_{P}) = E(f| P) = E(f |\\ f(x_j) = y_j \\text{ for } x_j\\in P)\n",
    "$$\n",
    "$$\n",
    "dist(f, S(f|_{P})) = \\max_x u(x)\n",
    "$$\n",
    "$$\n",
    "u(x) = Var(f| P)(x)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature distribution \n",
    "Let we have the measure $\\mu$ on $X$, $d\\mu = \\rho(x)dx$, that is the probability distribution of features $x$ on $X$. \n",
    "\n",
    "We want to find $x^*$ that is maximized $u(x^*) \\rho(x^*)$. So define:\n",
    "$$\n",
    "dist(f, S(f|_{P})) = \\max_x \\rho(x) u(x)\n",
    "$$\n",
    "\n",
    "Why we use such function:\n",
    "\n",
    "If the uncertainty u(x) in $x$ is not less then threshold $u_{max}$ then we should send $x$ to the oracle to define the exact value of $f(x)$. We want to minimize the process time of the oracle. So we want to minimize\n",
    "$$\n",
    "P_\\mu(u(x)>u_{max}) = \\int 1_{u(x)>u_{max}} \\rho(x) dx\n",
    "$$\n",
    "We have:\n",
    "$$\n",
    "P_\\mu(u(x)>u_{max}) \\approx \\int u(x) \\rho(x) dx\n",
    "$$\n",
    "$$\n",
    "\\delta P_\\mu(u(x)>u_{max}) = \\int \\delta u(x) \\rho(x) dx\n",
    "$$\n",
    "$$\n",
    "\\delta u(x) \\approx \\begin{cases}\n",
    "  u(x^*), & \\text{if } x = x^*, \\\\\n",
    "  0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "\\delta P_\\mu(u(x)>u_{max}) = u(x^*) \\rho(x^*)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension problem\n",
    "The dimension of $X$ is very large that is not applicable to find an approximation function with small error. \n",
    "\n",
    "##### Case: reversible embedding\n",
    "\n",
    "If there is an embedding: $Em: X \\to \\tilde X $ with revesible map $Em^{-1}: \\tilde X \\to X $. Then we can work in  $\\tilde X$.\n",
    "\n",
    "##### Case: separated features\n",
    "\n",
    "Let $x=(m,s)$ such that $\\rho(x) = \\rho_m(m) \\rho_s(s)$ where $\\rho_s(s) = \\frac 1 N \\sum_j \\delta(s_j)$ then we have $N$ active learning problems in the space of the dimension of variable $m$.\n",
    "$$\n",
    "dist = \\max_{s_j} \\max_{m} \\rho_m(m) u(m, s_j)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
